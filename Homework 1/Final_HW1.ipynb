{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Final_HW1.ipynb","provenance":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"jGfT6zFiEBcx"},"source":["# Marcer-Marchisciana\n","**Andrea Marcer - 10537040**\n","\n","**Matteo Marchisciana - 10586574**\n","\n","In this homework, we tried different models and approaches to tackle the problem. The solutions we tested are:\n","- Custom models\n","- Face detection approach\n","- Two steps approach\n","- Transfer learning\n"," - VGG16\n"," - *VGG16 with regularization*\n"," - Xception\n"," - NASNet Mobile\n","\n","Here we give a brief description of each approach and why we used it. More info will be written at the start of each section.\n","\n","### Custom models\n","First, we tried to take inspiration from VGG16 architecture, copying the main structure, but reducing number of layers and filters to avoid overfitting since the VGG network is very complex. However, we only reached a record of ~85% on validation accuracy.\n","\n","### Face detection approach\n","We thought that feeding directly the images to the CNN wasn't meaningful enought to classify correctly the dataset. In order to have higher possibility to classify the image we could detect all the faces and understand if they are wearing a mask or not. To do so we used an external Neural Network (MTCNN) to perform face detection; once we had the location of the faces we cropped each of them and fed them to the mask/no mask CNN. Even if this latter CNN was performing very well on the training and validation set (~96% accuracy) the MTCNN wasn't able to detect some faces or was detecting background. The overall performance was ~87% on the test using the training dataset of the homework.\n","\n","### Two steps approach\n","For the same reason as the Face detection approach, we designed a two-steps approach. The first step is to use a CNN to detect if there is at least one mask in the image and, if the response is affermative (it detects at least one mask), the second step is to distinguish if there is at least a face without a mask. For this reason we tried two different CNN on the homework dataset, based off of VGG16. The first CNN (mask vs nomask) performed really well on validation (~97%) but, expectedly, the second one did not match its performance, scoring ~91%. This approach performed ~86% on the Kaggle test.\n","\n","### Transfer learning\n","We tried different models for transfer learning. We started with VGG16 as seen in the lessons, then we tested Xception by Google, and NASNet mobile. The best performance, however, was initially reached with VGG16, so we decided to focus and spend more time optimizing the parameters for this model.\n","We also tried to use the VGG16 preprocessing function, but the model performed equal or better to our preprocessing.\n","The best result was obtained with:\n","- lambda_conv = 0.001\n","- freeze_until = 6\n","- learning_rate = 1e-5\n","- Dropout rate = 0.3\n","- Epoch = 23\n","- Two dense layer of 512 units, Dropout, Softmax\n","\n","# Overfitting\n","The main issue we encountered was overfitting. The validation accuracy often remained around ~85% while the training accuracy was ~97% and training loss was very low. For this reason nearly every approach we followed was aimed at reducing overfitting.\n","To do so we used Dropout in the top part of the network, we introduced kernel and bias regularizers in Convolutional layers, and tried different configuration of fine tuning. Our main effort was spent in researching and optimizing these parameters.\n","\n","### Data augmentation\n","We initially started training the models without data augmentation, but we soon found out that applying it was another method to reduce overfitting. The parameter we used for the final model are \n"," - rotation_range=30\n"," - width_shift_range=25\n"," - height_shift_range=25\n"," - zoom_range=0.2\n"," - horizontal_flip=True\n","\n","We did not want to go too far with transformations because we feared it might impact the label of the image (for example, maybe shifting the image too far would crop a face).\n","\n","## Attachments\n","To better understand our work, we thought it was better to include certain files used in this homework. The file we included are:\n","- *TestModel.py*: the script we used to generate the CSV from the predictions of the test set. We took these CSVs and submitted them on Kaggle to evaluate our predictions.\n","\n","- *TestModelTwoSteps.py*: this is the variant of the TestModel.py script we used to evaluate our Two Steps Approach.\n","\n","- *FinalModel.py*: the script that generated the best performance, both on validation set and Kaggle test.\n","\n","- *VGG16Imagenet_regularized.py*: this script defines a single function and returns the VGG16 model with regularization and imagenet weights. It is called by FinalModel.py\n","\n","- *TrainingSplitter.py*: the script that splits the training set in training and validation dataset, both labeled, allowing to use the flow_from_directory method."]},{"cell_type":"markdown","metadata":{"id":"Ue6S4hCazoPY"},"source":["# Initialization"]},{"cell_type":"code","metadata":{"id":"azMF_fh8f-2T"},"source":["!sudo pip install mtcnn"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tUKkbzHFUZei"},"source":["import tensorflow as tf\n","import numpy as np\n","import pandas as pd\n","import os\n","import cv2\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","from mtcnn.mtcnn import MTCNN\n","from sklearn.metrics import confusion_matrix, classification_report\n","from keras_preprocessing.image import ImageDataGenerator\n","from keras.models import Sequential\n","from keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout\n","from keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard\n","from keras.regularizers import l1, l2, l1_l2\n","from datetime import datetime\n","from PIL import Image\n","from google.colab import drive"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"02Nkbxj-Glzk"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Fj4UriglzWxO"},"source":["################ Settings ################\n","SEED = 123456789\n","\n","verbose = 0;\n","\n","# Current Working Directory\n","cwd = '/content/drive/MyDrive/POLI/Artificial Neural Networks and Deep Learning/Homeworks/Homework1/'\n","##########################################\n","\n","tf.random.set_seed(SEED)\n","np.random.seed(SEED)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NorDU43BFkyJ"},"source":["# Custom\n","With this model we took inspiration from the VGG16's architecture. The custom model has 10 Convolutional layers divided in 5 blocks, with MaxPooling layer at the end of each block. In the last part of the network we have a flatten layer and one dense layer with 512 neurons. We added one Dropout layer to avoid overfitting."]},{"cell_type":"markdown","metadata":{"id":"ma8Vc0ILHg1k"},"source":["## Dataset"]},{"cell_type":"code","metadata":{"id":"B7kIqOJ2Hg1k"},"source":["################ Settings ################\n","# Input image shape\n","img_height = 256 \n","img_width = 256\n","img_channels = 3\n","\n","# Directories\n","dataset_dir = os.path.join(cwd, 'homework1-dataset')\n","training_dir = os.path.join(dataset_dir, 'training')\n","validation_dir = os.path.join(dataset_dir, 'training')\n","test_dir = os.path.join(dataset_dir, 'test')\n","##########################################"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AdX7YXF_y0Ge"},"source":["#### Dataframe"]},{"cell_type":"code","metadata":{"id":"GpSVd1p0y2-F","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606041465034,"user_tz":-60,"elapsed":2068,"user":{"displayName":"Andrea Marcer","photoUrl":"https://lh6.googleusercontent.com/-y4YuyDO4-OI/AAAAAAAAAAI/AAAAAAAATsY/92XW3ePC6aE/s64/photo.jpg","userId":"09902568168977882946"}},"outputId":"0ad1ce2d-bb97-4887-9e79-397af499fa67"},"source":["################ Settings ################\n","json_path = os.path.join(dataset_dir, 'train_gt.json')\n","##########################################\n","\n","json = pd.read_json(lines=True, path_or_buf=json_path)\n","\n","# Create Data Frame \n","df = pd.DataFrame(json)\n","# First row is the label of the file\n","df = df.rename(index={0: \"label\"})\n","# Transpose the dataframe\n","df = df.T\n","# Convert elements to strings \n","df['file'] = df.index.astype(str)\n","df['label'] = df['label'].astype(str)\n","# Shuffle the dataframe\n","df = df.sample(frac=1)\n","# Set number of classes as the number of different values in the label column\n","num_classes = df.groupby('label').count().T.columns.size\n","\n","print(df.groupby('label').count())\n","print(\"\\n\\n\")\n","print(df)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["       file\n","label      \n","0      1900\n","1      1897\n","2      1817\n","\n","\n","\n","          label       file\n","16201.jpg     0  16201.jpg\n","10306.jpg     1  10306.jpg\n","16294.jpg     1  16294.jpg\n","14489.jpg     0  14489.jpg\n","15982.jpg     1  15982.jpg\n","...         ...        ...\n","10714.jpg     0  10714.jpg\n","10787.jpg     0  10787.jpg\n","16311.jpg     2  16311.jpg\n","15736.jpg     1  15736.jpg\n","14511.jpg     1  14511.jpg\n","\n","[5614 rows x 2 columns]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"i6k2gTDoHg1k"},"source":["### Training Dataset"]},{"cell_type":"code","metadata":{"id":"h6a1rjs6Hg1k","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606041468576,"user_tz":-60,"elapsed":2817,"user":{"displayName":"Andrea Marcer","photoUrl":"https://lh6.googleusercontent.com/-y4YuyDO4-OI/AAAAAAAAAAI/AAAAAAAATsY/92XW3ePC6aE/s64/photo.jpg","userId":"09902568168977882946"}},"outputId":"09e2dc37-a47a-43b6-b085-6e99de0b1c98"},"source":["################ Settings ################\n","apply_data_augmentation = False\n","batch_size = 64\n","validation_split = 0.2\n","##########################################\n","\n","# ImageDataGenerator\n","if apply_data_augmentation:\n","    train_data_gen = ImageDataGenerator(\n","        rotation_range = 10,\n","        width_shift_range = 10,\n","        height_shift_range = 10,\n","        zoom_range = 0.3,\n","        horizontal_flip=True,\n","        # vertical_flip=True,\n","        fill_mode = \"nearest\",\n","        cval=0,\n","        rescale=1./255,# All pixels in the range 0-1\n","        validation_split=validation_split) \n","else:\n","    train_data_gen = ImageDataGenerator(\n","        rescale=1./255,\n","        validation_split=validation_split)\n","    \n","train_gen=train_data_gen.flow_from_dataframe(\n","  dataframe = df,\n","  directory = training_dir,\n","  x_col = \"file\",\n","  y_col = \"label\",\n","  subset = \"training\",\n","  batch_size = batch_size,\n","  seed = SEED,\n","  shuffle = True,\n","  class_mode = \"categorical\",\n","  target_size = (img_height, img_width)\n",")\n","\n","train_dataset = tf.data.Dataset.from_generator(\n","    lambda: train_gen,\n","    output_types=(tf.float32, tf.float32),\n","    output_shapes=([None, img_height, img_width, img_channels], [None, num_classes])\n",")\n","\n","train_dataset = train_dataset.repeat()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Found 4492 validated image filenames belonging to 3 classes.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"BskvNqJhHg1l"},"source":["### Validation Dataset"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZHOiDTOMHg1l","executionInfo":{"status":"ok","timestamp":1606041475260,"user_tz":-60,"elapsed":4198,"user":{"displayName":"Andrea Marcer","photoUrl":"https://lh6.googleusercontent.com/-y4YuyDO4-OI/AAAAAAAAAAI/AAAAAAAATsY/92XW3ePC6aE/s64/photo.jpg","userId":"09902568168977882946"}},"outputId":"23b8cf2c-9e99-4beb-a84f-9e4a389f1487"},"source":["#valid_data_gen = ImageDataGenerator(rescale=1./255)\n","\n","valid_gen = train_data_gen.flow_from_dataframe(\n","    dataframe=df,\n","    directory = validation_dir,\n","    x_col=\"file\",\n","    y_col=\"label\",\n","    target_size = (img_height, img_width),\n","    class_mode = \"categorical\",\n","    seed = SEED,\n","    subset = \"validation\",\n","    batch_size = batch_size\n",")\n","\n","valid_dataset = tf.data.Dataset.from_generator(\n","    lambda: valid_gen, \n","    output_types=(tf.float32, tf.float32),\n","    output_shapes=([None, img_height, img_width, img_channels], [None, num_classes]))\n","\n","valid_dataset = valid_dataset.repeat()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Found 1122 validated image filenames belonging to 3 classes.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_MQRHkLVHg1l"},"source":["## Model"]},{"cell_type":"code","metadata":{"id":"18yFa0lAHg1l"},"source":["################ Model parameters ################\n","activation_function = 'selu'\n","padding = 'same'\n","pool_size = (2, 2)\n","output_activation_function = 'softmax'\n","input_shape = (img_height, img_width, img_channels)\n","output_neurons = num_classes\n","#####################################################"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"I176P-OJmbiM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606041477322,"user_tz":-60,"elapsed":898,"user":{"displayName":"Andrea Marcer","photoUrl":"https://lh6.googleusercontent.com/-y4YuyDO4-OI/AAAAAAAAAAI/AAAAAAAATsY/92XW3ePC6aE/s64/photo.jpg","userId":"09902568168977882946"}},"outputId":"409d95e9-7ea2-4a2f-d763-2d46632000d3"},"source":["model = Sequential()\n","\n","model.add(Conv2D(64, (5,5), activation=activation_function, padding=padding, input_shape=input_shape))\n","model.add(Conv2D(64, (3,3), activation=activation_function, padding=padding))\n","model.add(MaxPooling2D(pool_size=pool_size))\n","\n","model.add(Conv2D(128, (3,3), activation=activation_function, padding=padding))\n","model.add(Conv2D(128, (3,3), activation=activation_function, padding=padding))\n","model.add(MaxPooling2D(pool_size=pool_size))\n","\n","model.add(Conv2D(256, (3,3), activation=activation_function, padding=padding))\n","model.add(Conv2D(256, (3,3), activation=activation_function, padding=padding))\n","model.add(MaxPooling2D(pool_size=pool_size))\n","\n","model.add(Conv2D(512, (3,3), activation=activation_function, padding=padding))\n","model.add(Conv2D(512, (3,3), activation=activation_function, padding=padding))\n","model.add(MaxPooling2D(pool_size=pool_size))\n","\n","model.add(Conv2D(512, (3,3), activation=activation_function, padding=padding))\n","model.add(Conv2D(512, (3,3), activation=activation_function, padding=padding))\n","model.add(MaxPooling2D(pool_size=pool_size))\n","\n","model.add(Flatten())\n","model.add(Dense(512, activation=activation_function))\n","model.add(Dropout(rate = 0.2))\n","model.add(Dense(output_neurons, activation=output_activation_function))\n","\n","model.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv2d (Conv2D)              (None, 256, 256, 64)      4864      \n","_________________________________________________________________\n","conv2d_1 (Conv2D)            (None, 256, 256, 64)      36928     \n","_________________________________________________________________\n","max_pooling2d (MaxPooling2D) (None, 128, 128, 64)      0         \n","_________________________________________________________________\n","conv2d_2 (Conv2D)            (None, 128, 128, 128)     73856     \n","_________________________________________________________________\n","conv2d_3 (Conv2D)            (None, 128, 128, 128)     147584    \n","_________________________________________________________________\n","max_pooling2d_1 (MaxPooling2 (None, 64, 64, 128)       0         \n","_________________________________________________________________\n","conv2d_4 (Conv2D)            (None, 64, 64, 256)       295168    \n","_________________________________________________________________\n","conv2d_5 (Conv2D)            (None, 64, 64, 256)       590080    \n","_________________________________________________________________\n","max_pooling2d_2 (MaxPooling2 (None, 32, 32, 256)       0         \n","_________________________________________________________________\n","conv2d_6 (Conv2D)            (None, 32, 32, 512)       1180160   \n","_________________________________________________________________\n","conv2d_7 (Conv2D)            (None, 32, 32, 512)       2359808   \n","_________________________________________________________________\n","max_pooling2d_3 (MaxPooling2 (None, 16, 16, 512)       0         \n","_________________________________________________________________\n","conv2d_8 (Conv2D)            (None, 16, 16, 512)       2359808   \n","_________________________________________________________________\n","conv2d_9 (Conv2D)            (None, 16, 16, 512)       2359808   \n","_________________________________________________________________\n","max_pooling2d_4 (MaxPooling2 (None, 8, 8, 512)         0         \n","_________________________________________________________________\n","flatten (Flatten)            (None, 32768)             0         \n","_________________________________________________________________\n","dense (Dense)                (None, 512)               16777728  \n","_________________________________________________________________\n","dropout (Dropout)            (None, 512)               0         \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 3)                 1539      \n","=================================================================\n","Total params: 26,187,331\n","Trainable params: 26,187,331\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"UsiUT5hbHg1l"},"source":["## Training"]},{"cell_type":"code","metadata":{"id":"oZuuGNRIHg1l"},"source":["################ Optimization params ################ \n","loss = tf.keras.losses.CategoricalCrossentropy()\n","learning_rate = 1e-4\n","optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n","metrics = ['accuracy']\n","#####################################################\n","\n","model.compile(optimizer=optimizer, loss=loss, metrics=metrics)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZSl_FIfeHg1l"},"source":["################ Settings ################\n","# Tensorboard\n","tensorBoard = False\n","\n","# Early stopping\n","earlyStopping = True\n","patience = 10\n","\n","# Model check\n","checkpoints_dir = os.path.join(cwd, 'checkpoints')\n","modelCheckpoint = False\n","##########################################\n","\n","callback_list = []\n","\n","if (earlyStopping):\n","  es = EarlyStopping(\n","      monitor = 'val_loss',\n","      min_delta = 0, \n","      patience = patience,\n","      verbose = verbose,\n","      mode = 'auto',\n","      baseline = None,\n","      restore_best_weights = True)\n","  callback_list.append(es)\n","\n","if (modelCheckpoint):\n","  cp = ModelCheckpoint(\n","      checkpoints_dir,\n","      monitor = 'val_loss',\n","      verbose = verbose,\n","      save_best_only = False,\n","      save_weights_only = False,\n","      mode = 'auto',\n","      save_freq = 1)\n","  callback_list.append(cp)\n","\n","if (tensorBoard):\n","  tb = TensorBoard(\n","      log_dir='/content/tb_log',\n","      profile_batch=0,\n","      histogram_freq=1, # if 1 shows weights histograms\n","      ) \n","  callback_list.append(tb)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"h2BJXMqKHg1l","colab":{"base_uri":"https://localhost:8080/"},"outputId":"29e1abb0-12de-4e76-c9c6-2ba9e0d40bb5"},"source":["################ Settings ################\n","epochs = 300\n","##########################################\n"," \n","model.fit(\n","    x = train_dataset,\n","    epochs = epochs,\n","    callbacks = callback_list,\n","    steps_per_epoch = len(train_gen), \n","    validation_data = valid_dataset,\n","    validation_steps = len(valid_gen)\n",")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/300\n"," 1/71 [..............................] - ETA: 0s - loss: 2.0654 - accuracy: 0.2969"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Xk_X1v0HGYfs"},"source":["# VGG\n","With this model we used the transfer learning approach to copy the bottom structure and the weights of the VGG16. We tried to append one and two Dense layers before finally adding the last Dense layer with softmax activation.\n","Using fine tuning, we tried different configuration of frozen layers, learning rates and activation functions. When it was clear that this was the approach that performed better, we added an extra regularization methods: we hand-wrote the model in python and downloaded and loaded the weight. Doing so allowed us to add, in each convolutional layer, a kernel and a bias regularizer. We thought that doing so will help to avoid overfitting, since the l2 function penalizes big weights. It was then necessary to research the optimal lambda parameter.\n","In this approach we used a script to split training and validation sets and put them in labeled folder, in order to use the flow_from_directory method of ImageDataGenerator. Since it was done in local, we will write here a version that uses flow_from_dataframe. However, the script is used is attached in the zip file.\n","\n","The best result was obtained with:\n","- lambda_conv = 0.001\n","- freeze_until = 6\n","- learning_rate = 1e-5\n","- Dropout rate = 0.3\n","- Epoch = 23\n","- Two dense layer of 512 units, Dropout, Softmax"]},{"cell_type":"markdown","metadata":{"id":"UC4uMAZj28X-"},"source":["## Dataset"]},{"cell_type":"code","metadata":{"id":"2NumiKF828X-"},"source":["################ Settings ################\n","# Input image shape\n","img_height = 256 \n","img_width = 256\n","img_channels = 3\n","\n","# Directories\n","dataset_dir = os.path.join(cwd, 'homework1-dataset')\n","training_dir = os.path.join(dataset_dir, 'training')\n","validation_dir = os.path.join(dataset_dir, 'training')\n","test_dir = os.path.join(dataset_dir, 'test')\n","##########################################"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"affIoMR928X-"},"source":["#### Dataframe"]},{"cell_type":"code","metadata":{"id":"16u-NW_P28X-"},"source":["################ Settings ################\n","json_path = os.path.join(dataset_dir, 'train_gt.json')\n","##########################################\n","\n","json = pd.read_json(lines=True, path_or_buf=json_path)\n","\n","# Create Data Frame \n","df = pd.DataFrame(json)\n","# First row is the label of the file\n","df = df.rename(index={0: \"label\"})\n","# Transpose the dataframe\n","df = df.T\n","# Convert elements to strings \n","df['file'] = df.index.astype(str)\n","df['label'] = df['label'].astype(str)\n","# Shuffle the dataframe\n","df = df.sample(frac=1)\n","# Set number of classes as the number of different values in the label column\n","num_classes = df.groupby('label').count().T.columns.size\n","\n","print(df.groupby('label').count())\n","print(\"\\n\\n\")\n","print(df)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ICF8PIzF28X_"},"source":["### Training Dataset"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7-UZK9qx28X_","executionInfo":{"status":"ok","timestamp":1606041468576,"user_tz":-60,"elapsed":2817,"user":{"displayName":"Andrea Marcer","photoUrl":"https://lh6.googleusercontent.com/-y4YuyDO4-OI/AAAAAAAAAAI/AAAAAAAATsY/92XW3ePC6aE/s64/photo.jpg","userId":"09902568168977882946"}},"outputId":"09e2dc37-a47a-43b6-b085-6e99de0b1c98"},"source":["################ Settings ################\n","apply_data_augmentation = True\n","batch_size = 32\n","validation_split = 0.2\n","##########################################\n","\n","# ImageDataGenerator\n","if apply_data_augmentation:\n","    train_data_gen = ImageDataGenerator(\n","        rotation_range=20,\n","        width_shift_range=25,\n","        height_shift_range=25,\n","        zoom_range=0.2,\n","        horizontal_flip=True,\n","        rescale=1. / 255,\n","        validation_split=validation_split\n","        ) \n","else:\n","    train_data_gen = ImageDataGenerator(\n","        rescale=1./255,\n","        validation_split=validation_split)\n","    \n","train_gen=train_data_gen.flow_from_dataframe(\n","  dataframe = df,\n","  directory = training_dir,\n","  x_col = \"file\",\n","  y_col = \"label\",\n","  subset = \"training\",\n","  batch_size = batch_size,\n","  seed = SEED,\n","  shuffle = True,\n","  class_mode = \"categorical\",\n","  target_size = (img_height, img_width)\n",")\n","\n","train_dataset = tf.data.Dataset.from_generator(\n","    lambda: train_gen,\n","    output_types=(tf.float32, tf.float32),\n","    output_shapes=([None, img_height, img_width, img_channels], [None, num_classes])\n",")\n","\n","train_dataset = train_dataset.repeat()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Found 4492 validated image filenames belonging to 3 classes.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"0Hp5ZaQG28X_"},"source":["### Validation Dataset"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DfsJ6zxD28X_","executionInfo":{"status":"ok","timestamp":1606041475260,"user_tz":-60,"elapsed":4198,"user":{"displayName":"Andrea Marcer","photoUrl":"https://lh6.googleusercontent.com/-y4YuyDO4-OI/AAAAAAAAAAI/AAAAAAAATsY/92XW3ePC6aE/s64/photo.jpg","userId":"09902568168977882946"}},"outputId":"23b8cf2c-9e99-4beb-a84f-9e4a389f1487"},"source":["#valid_data_gen = ImageDataGenerator(rescale=1./255)\n","\n","valid_gen = train_data_gen.flow_from_dataframe(\n","    dataframe=df,\n","    directory = validation_dir,\n","    x_col=\"file\",\n","    y_col=\"label\",\n","    target_size = (img_height, img_width),\n","    class_mode = \"categorical\",\n","    seed = SEED,\n","    subset = \"validation\",\n","    batch_size = batch_size\n",")\n","\n","valid_dataset = tf.data.Dataset.from_generator(\n","    lambda: valid_gen, \n","    output_types=(tf.float32, tf.float32),\n","    output_shapes=([None, img_height, img_width, img_channels], [None, num_classes]))\n","\n","valid_dataset = valid_dataset.repeat()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Found 1122 validated image filenames belonging to 3 classes.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"w-iBLbsk28X_"},"source":["## Model"]},{"cell_type":"code","metadata":{"id":"YBXZAkeh28X_"},"source":["############ Structure and weight of VGG16 with regularization #################\n","\n","lambda_conv = 0.001\n","imagenet_path = \"vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\"\n","\n","\n","def VGG16Imagenet_Regularized():\n","    model = keras.Sequential()\n","\n","    model.add(layers.Conv2D(64, (3, 3),\n","                            activation='relu',\n","                            padding='same',\n","                            kernel_regularizer=l2(lambda_conv),\n","                            bias_regularizer=l2(lambda_conv),\n","                            name='block1_conv1',\n","                            input_shape=(256, 256, 3)))\n","    model.add(layers.Conv2D(64, (3, 3),\n","                            activation='relu',\n","                            padding='same',\n","                            kernel_regularizer=l2(lambda_conv),\n","                            bias_regularizer=l2(lambda_conv),\n","                            name='block1_conv2'))\n","    model.add(layers.MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool'))\n","\n","    # Block 2\n","    model.add(layers.Conv2D(128, (3, 3),\n","                            activation='relu',\n","                            padding='same',\n","                            kernel_regularizer=l2(lambda_conv),\n","                            bias_regularizer=l2(lambda_conv),\n","                            name='block2_conv1'))\n","    model.add(layers.Conv2D(128, (3, 3),\n","                            activation='relu',\n","                            padding='same',\n","                            kernel_regularizer=l2(lambda_conv),\n","                            bias_regularizer=l2(lambda_conv),\n","                            name='block2_conv2'))\n","    model.add(layers.MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool'))\n","\n","    # Block 3\n","    model.add(layers.Conv2D(256, (3, 3),\n","                            activation='relu',\n","                            padding='same',\n","                            kernel_regularizer=l2(lambda_conv),\n","                            bias_regularizer=l2(lambda_conv),\n","                            name='block3_conv1'))\n","    model.add(layers.Conv2D(256, (3, 3),\n","                            activation='relu',\n","                            padding='same',\n","                            kernel_regularizer=l2(lambda_conv),\n","                            bias_regularizer=l2(lambda_conv),\n","                            name='block3_conv2'))\n","    model.add(layers.Conv2D(256, (3, 3),\n","                            activation='relu',\n","                            padding='same',\n","                            kernel_regularizer=l2(lambda_conv),\n","                            bias_regularizer=l2(lambda_conv),\n","                            name='block3_conv3'))\n","    model.add(layers.MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool'))\n","\n","    # Block 4\n","    model.add(layers.Conv2D(512, (3, 3),\n","                            activation='relu',\n","                            padding='same',\n","                            kernel_regularizer=l2(lambda_conv),\n","                            bias_regularizer=l2(lambda_conv),\n","                            name='block4_conv1'))\n","    model.add(layers.Conv2D(512, (3, 3),\n","                            activation='relu',\n","                            padding='same',\n","                            kernel_regularizer=l2(lambda_conv),\n","                            bias_regularizer=l2(lambda_conv),\n","                            name='block4_conv2'))\n","    model.add(layers.Conv2D(512, (3, 3),\n","                            activation='relu',\n","                            padding='same',\n","                            kernel_regularizer=l2(lambda_conv),\n","                            bias_regularizer=l2(lambda_conv),\n","                            name='block4_conv3'))\n","    model.add(layers.MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool'))\n","\n","    # Block 5\n","    model.add(layers.Conv2D(512, (3, 3),\n","                            activation='relu',\n","                            padding='same',\n","                            kernel_regularizer=l2(lambda_conv),\n","                            bias_regularizer=l2(lambda_conv),\n","                            name='block5_conv1'))\n","    model.add(layers.Conv2D(512, (3, 3),\n","                            activation='relu',\n","                            padding='same',\n","                            kernel_regularizer=l2(lambda_conv),\n","                            bias_regularizer=l2(lambda_conv),\n","                            name='block5_conv2'))\n","    model.add(layers.Conv2D(512, (3, 3),\n","                            activation='relu',\n","                            padding='same',\n","                            kernel_regularizer=l2(lambda_conv),\n","                            bias_regularizer=l2(lambda_conv),\n","                            name='block5_conv3'))\n","    model.add(layers.MaxPooling2D((2, 2), strides=(2, 2), name='block5_pool'))\n","\n","    model.load_weights(imagenet_path)\n","\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4_8ZrMby28X_","executionInfo":{"status":"ok","timestamp":1606041477322,"user_tz":-60,"elapsed":898,"user":{"displayName":"Andrea Marcer","photoUrl":"https://lh6.googleusercontent.com/-y4YuyDO4-OI/AAAAAAAAAAI/AAAAAAAATsY/92XW3ePC6aE/s64/photo.jpg","userId":"09902568168977882946"}},"outputId":"409d95e9-7ea2-4a2f-d763-2d46632000d3"},"source":["model = VGG16Imagenet_Regularized()\n","model.add(tf.keras.layers.Flatten())\n","\n","model.add(tf.keras.layers.Dense(units=512,\n","                                activation=\"selu\",\n","                                kernel_regularizer=l2(0.001),\n","                                bias_regularizer=l2(0.001)))\n","model.add(tf.keras.layers.Dense(units=512,\n","                                activation=\"selu\",\n","                                kernel_regularizer=l2(0.001),\n","                                bias_regularizer=l2(0.001)))\n","model.add(tf.keras.layers.Dropout(rate=0.3))\n","model.add(tf.keras.layers.Dense(units=3,\n","                                activation=\"softmax\"))\n","\n","enable_finetuning = True\n","\n","if enable_finetuning:\n","    freeze_until = 6  # Layer from which we want to fine-tune.\n","    for layer in model.layers[:freeze_until]:\n","        layer.trainable = False\n","else:\n","    model.trainable = False\n","\n","model.compile(\n","    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n","    loss=tf.keras.losses.CategoricalCrossentropy(),\n","    metrics=[\"accuracy\"]\n",")\n","\n","print(model.summary())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv2d (Conv2D)              (None, 256, 256, 64)      4864      \n","_________________________________________________________________\n","conv2d_1 (Conv2D)            (None, 256, 256, 64)      36928     \n","_________________________________________________________________\n","max_pooling2d (MaxPooling2D) (None, 128, 128, 64)      0         \n","_________________________________________________________________\n","conv2d_2 (Conv2D)            (None, 128, 128, 128)     73856     \n","_________________________________________________________________\n","conv2d_3 (Conv2D)            (None, 128, 128, 128)     147584    \n","_________________________________________________________________\n","max_pooling2d_1 (MaxPooling2 (None, 64, 64, 128)       0         \n","_________________________________________________________________\n","conv2d_4 (Conv2D)            (None, 64, 64, 256)       295168    \n","_________________________________________________________________\n","conv2d_5 (Conv2D)            (None, 64, 64, 256)       590080    \n","_________________________________________________________________\n","max_pooling2d_2 (MaxPooling2 (None, 32, 32, 256)       0         \n","_________________________________________________________________\n","conv2d_6 (Conv2D)            (None, 32, 32, 512)       1180160   \n","_________________________________________________________________\n","conv2d_7 (Conv2D)            (None, 32, 32, 512)       2359808   \n","_________________________________________________________________\n","max_pooling2d_3 (MaxPooling2 (None, 16, 16, 512)       0         \n","_________________________________________________________________\n","conv2d_8 (Conv2D)            (None, 16, 16, 512)       2359808   \n","_________________________________________________________________\n","conv2d_9 (Conv2D)            (None, 16, 16, 512)       2359808   \n","_________________________________________________________________\n","max_pooling2d_4 (MaxPooling2 (None, 8, 8, 512)         0         \n","_________________________________________________________________\n","flatten (Flatten)            (None, 32768)             0         \n","_________________________________________________________________\n","dense (Dense)                (None, 512)               16777728  \n","_________________________________________________________________\n","dropout (Dropout)            (None, 512)               0         \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 3)                 1539      \n","=================================================================\n","Total params: 26,187,331\n","Trainable params: 26,187,331\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"IIfTsa6A28X_"},"source":["## Training"]},{"cell_type":"code","metadata":{"id":"f_d-OXdS28X_"},"source":["################ Settings ################\n","# Tensorboard\n","tensorBoard = False\n","\n","# Early stopping\n","earlyStopping = True\n","patience = 20\n","\n","# Model check\n","checkpoints_dir = os.path.join(cwd, 'checkpoints')\n","modelCheckpoint = False\n","##########################################\n","\n","callback_list = []\n","\n","if (earlyStopping):\n","  es = EarlyStopping(\n","      monitor = 'val_accuracy',\n","      min_delta = 0, \n","      patience = patience,\n","      verbose = verbose,\n","      mode = 'auto',\n","      baseline = None,\n","      restore_best_weights = True)\n","  callback_list.append(es)\n","\n","if (modelCheckpoint):\n","  cp = ModelCheckpoint(\n","      checkpoints_dir,\n","      monitor = 'val_accuracy',\n","      verbose = verbose,\n","      save_best_only = False,\n","      save_weights_only = False,\n","      mode = 'auto',\n","      save_freq = \"epoch\")\n","  callback_list.append(cp)\n","\n","if (tensorBoard):\n","  tb = TensorBoard(\n","      log_dir='/content/tb_log',\n","      profile_batch=0,\n","      histogram_freq=1, # if 1 shows weights histograms\n","      ) \n","  callback_list.append(tb)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bjodHukp28X_","outputId":"29e1abb0-12de-4e76-c9c6-2ba9e0d40bb5"},"source":["################ Settings ################\n","epochs = 300\n","##########################################\n"," \n","model.fit(\n","    x = train_dataset,\n","    epochs = 300,\n","    callbacks = callback_list,\n","    steps_per_epoch = len(train_gen), \n","    validation_data = valid_dataset,\n","    validation_steps = len(valid_gen)\n",")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/300\n"," 1/71 [..............................] - ETA: 0s - loss: 2.0654 - accuracy: 0.2969"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"vkdvtdw5G2s2"},"source":["# Xception\n","Same procedure of the VGG16 but with different architeture. With this model we tried a few optimization steps, but after a while we saw that it did not surpass the performance of VGG16, so we decided to focus our effort on VGG16."]},{"cell_type":"markdown","metadata":{"id":"QBKZ-gW-LFGG"},"source":["## Dataset"]},{"cell_type":"code","metadata":{"id":"znAxmjk_LFGG"},"source":["################ Settings ################\n","# Input image shape\n","img_height = 224 \n","img_width = 224\n","img_channels = 3\n","\n","# Directories\n","dataset_dir = os.path.join(cwd, 'Faces')\n","training_dir = os.path.join(dataset_dir, 'Train')\n","validation_dir = os.path.join(dataset_dir, 'Validation')\n","test_dir = os.path.join(dataset_dir, 'Test')\n","##########################################"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5ARJjPF0LFGG"},"source":["### Training Dataset"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VCMuNUoMLFGG","executionInfo":{"status":"ok","timestamp":1605952491483,"user_tz":-60,"elapsed":47208,"user":{"displayName":"Andrea Marcer","photoUrl":"https://lh6.googleusercontent.com/-y4YuyDO4-OI/AAAAAAAAAAI/AAAAAAAATsY/92XW3ePC6aE/s64/photo.jpg","userId":"09902568168977882946"}},"outputId":"c4aa1342-967d-4ca9-b960-5a6dff53af40"},"source":["################ Settings ################\n","apply_data_augmentation = True\n","batch_size = 32\n","num_classes = 3\n","##########################################\n","\n","# Create training ImageDataGenerator object\n","if apply_data_augmentation:\n","    train_data_gen = ImageDataGenerator(\n","        rotation_range=25,\n","        width_shift_range=20,\n","        height_shift_range=20,\n","        zoom_range=0.2,\n","        horizontal_flip=True,\n","        # vertical_flip=True,\n","        rescale=1. / 255)\n","else:\n","    train_data_gen = ImageDataGenerator(rescale=1./255)\n","\n","train_gen = train_data_gen.flow_from_directory(\n","      training_dir,\n","      batch_size=batch_size,\n","      class_mode='categorical',\n","      shuffle=True,\n","      seed=SEED,\n","      target_size = (img_height, img_width)\n",")\n","\n","train_dataset = tf.data.Dataset.from_generator(\n","      lambda: train_gen,\n","      output_types=(tf.float32, tf.float32),\n","      output_shapes=([None, img_height, img_width, img_channels], [None, num_classes])\n",")\n","\n","train_dataset = train_dataset.repeat()\n","\n","train_gen.class_indices"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Found 14693 images belonging to 3 classes.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["{'Background': 0, 'Mask': 1, 'NoMask': 2}"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"o_CIQnI3LFGG"},"source":["### Validation Dataset"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bnB_F4JaLFGG","executionInfo":{"status":"ok","timestamp":1605952495949,"user_tz":-60,"elapsed":28058,"user":{"displayName":"Andrea Marcer","photoUrl":"https://lh6.googleusercontent.com/-y4YuyDO4-OI/AAAAAAAAAAI/AAAAAAAATsY/92XW3ePC6aE/s64/photo.jpg","userId":"09902568168977882946"}},"outputId":"8229b6b5-2844-423c-f109-346819837e50"},"source":["valid_data_gen = ImageDataGenerator(rescale=1./255)\n","\n","valid_gen = valid_data_gen.flow_from_directory(\n","      validation_dir,\n","      batch_size=batch_size, \n","      class_mode='categorical',\n","      shuffle=False,\n","      seed=SEED,\n","      target_size = (img_height, img_width)\n",")\n","\n","valid_dataset = tf.data.Dataset.from_generator(\n","      lambda: valid_gen, \n","      output_types=(tf.float32, tf.float32),\n","      output_shapes=([None, img_height, img_width, img_channels], [None, num_classes])\n",")\n","\n","valid_dataset = valid_dataset.repeat()\n","\n","valid_gen.class_indices"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Found 1960 images belonging to 3 classes.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["{'Background': 0, 'Mask': 1, 'NoMask': 2}"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"markdown","metadata":{"id":"_bRHv4P9LFGG"},"source":["## Model"]},{"cell_type":"code","metadata":{"id":"kts7q2Z-LFGG"},"source":["xception = tf.keras.applications.Xception(\n","    input_shape=(224, 224, 3), \n","    include_top=False, \n","    weights='imagenet'\n",")\n","\n","model = tf.keras.models.Sequential()\n","model.add(xception)\n","model.add(tf.keras.layers.Flatten())\n","# model.add(tf.keras.layers.Dropout(rate=0.2))\n","model.add(tf.keras.layers.Dense(units=512,\n","                                activation=\"relu\",\n","                                kernel_regularizer=l2(lambda_conv),\n","                                bias_regularizer=l2(lambda_conv)))\n","model.add(tf.keras.layers.Dense(units=512,\n","                                activation=\"relu\",\n","                                kernel_regularizer=l2(lambda_conv),\n","                                bias_regularizer=l2(lambda_conv)))\n","model.add(tf.keras.layers.Dropout(rate=0.3))\n","model.add(tf.keras.layers.Dense(units=3,\n","                                activation=\"softmax\"))\n","\n","enable_finetuning = True\n","\n","if enable_finetuning:\n","    freeze_until = 40  # Layer from which we want to fine-tune.\n","    for layer in model.layers[:freeze_until]:\n","        layer.trainable = False\n","else:\n","    model.trainable = False\n","\n","model.compile(\n","    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n","    loss=tf.keras.losses.CategoricalCrossentropy(),\n","    metrics=[\"accuracy\"]\n",")\n","\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KPSiahuWLFGH"},"source":["## Training"]},{"cell_type":"code","metadata":{"id":"G9k03TX9LFGH"},"source":["################ Settings ################\n","# Tensorboard\n","tensorBoard = True\n","\n","# Early stopping\n","earlyStopping = True\n","patience = 20\n","\n","# Model check\n","checkpoints_dir = os.path.join(cwd, 'checkpoints')\n","modelCheckpoint = False\n","##########################################\n","\n","callback_list = []\n","\n","if (earlyStopping):\n","  es = EarlyStopping(\n","      monitor = 'val_accuracy',\n","      min_delta = 0, \n","      patience = patience,\n","      verbose = verbose,\n","      mode = 'auto',\n","      baseline = None,\n","      restore_best_weights = True)\n","  callback_list.append(es)\n","\n","if (modelCheckpoint):\n","  cp = ModelCheckpoint(\n","      checkpoints_dir,\n","      monitor = 'val_accuracy',\n","      verbose = verbose,\n","      save_best_only = False,\n","      save_weights_only = False,\n","      mode = 'auto',\n","      save_freq = \"epoch\")\n","  callback_list.append(cp)\n","\n","if (tensorBoard):\n","  tb = TensorBoard(\n","      log_dir='/content/tb_log',\n","      profile_batch=0,\n","      histogram_freq=1, # if 1 shows weights histograms\n","      ) \n","  callback_list.append(tb)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QyJzKmKlLFGH"},"source":["################ Settings ################\n","epochs = 300\n","##########################################\n"," \n","model.fit(\n","    x = train_dataset,\n","    epochs = epochs,\n","    callbacks = callback_list,\n","    steps_per_epoch = len(train_gen), \n","    validation_data = valid_dataset,\n","    validation_steps = len(valid_gen)\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0Q1X9vNVG5Pl"},"source":["# Two-steps architecture\n","We thought that it should be much easier to detect masks given their bright colors. Thus we trained two networks: one that detects if there is at least a mask and one that classifies between SomeMask and AllMask. During prediction we used the first model and then, given its prediction, we used the second network to detect if there is at least one person without a mask.\n","The two models used for the training are the same we used for the VGG16 approach, regularized.\n","\n","We attach only the script used to build the two dataframes, since the rest of the script is roughly the same as the VGG16 approach.\n","\n","As written in the introduction reference the file TestModelTwoSteps.py to see how the prediction has been performed."]},{"cell_type":"markdown","metadata":{"id":"TKg37XaL-wWU"},"source":["## Dataframes"]},{"cell_type":"code","metadata":{"id":"8MmlpzsO-zfD"},"source":["dfMaskvsNomask = pd.DataFrame(json)\n","dfMaskvsNomask = dfMaskvsNomask.rename(index={0: \"label\"})\n","dfMaskvsNomask = dfMaskvsNomask.T\n","dfMaskvsNomask['file'] = dfMaskvsNomask.index.astype(str)\n","dfMaskvsNomask['label'] = dfMaskvsNomask['label'].astype(str)\n","dfMaskvsNomask['label'].replace({\"2\": \"1\"}, inplace=True)\n","dfMaskvsNomask['label'] = dfMaskvsNomask['label'].astype(str)\n","dfMaskvsNomask = dfMaskvsNomask.sample(frac=1)\n","\n","dfAllmaskVSSomemask = pd.DataFrame(json)\n","dfAllmaskVSSomemask = dfAllmaskVSSomemask.rename(index={0: \"label\"})\n","dfAllmaskVSSomemask = dfAllmaskVSSomemask.T\n","dfAllmaskVSSomemask['file'] = dfAllmaskVSSomemask.index.astype(str)\n","dfAllmaskVSSomemask['label'] = dfAllmaskVSSomemask['label'].astype(str)\n","dfAllmaskVSSomemask['label'].replace({\"2\": \"0\"}, inplace=True)\n","dfAllmaskVSSomemask['label'] = dfAllmaskVSSomemask['label'].astype(str)\n","dfAllmaskVSSomemask = dfAllmaskVSSomemask.sample(frac=1)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3mL16kEyGx2V"},"source":["# Face detection MTCNN\n","The main idea is to use a face detector to locate faces, crop them, count how many faces have the mask and use this information to predict the class. In order to train the network that detects masks we cropped the faces of the orginal dataset using the face detector, saved these images, divided them in training and validation, cleaned up the images creating a new Background class to detect false positives of the detector. The mask detection network performed really well, but the face detector does not detect all the faces (probably due to the mask) so the overall result isn't good enough."]},{"cell_type":"markdown","metadata":{"id":"xsq5cTNqLv_P"},"source":["## Create Face Dataset"]},{"cell_type":"code","metadata":{"id":"eabJ3_jSKms7"},"source":["# Load the dataframe\n","json = pd.read_json(lines=True, path_or_buf='/content/drive/MyDrive/POLI/Artificial Neural Networks and Deep Learning/Homeworks/Homework1/homework1-dataset/train_gt.json')\n","\n","df = pd.DataFrame(json)\n","df = df.rename(index={0: \"label\"})\n","df = df.T\n","df['file'] = df.index.astype(str)\n","df['label'] = df['label'].astype(str)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bqn3yHGgQZS-"},"source":["offset = 10\n","\n","# Returns the array of cropped faces from the input image\n","def crop_faces(filename, result_list):\n","\tprint(\"cropping \" + str(len(result_list)))\n","\toriginal_img = Image.open(filename)\n","\timgs = []\n","\tfor i in range(len(result_list)):\n","\t\tx1, y1, width, height = result_list[i]['box']\n","\t\tx2, y2 = x1 + width, y1 + height\n","\t\timg = original_img.crop((x1-offset, y1-offset, x2+offset,  y2+offset))\n","\t\timg = img.resize((224,224), Image.ANTIALIAS)\n","\t\timgs.append(img)\n","\treturn imgs\n","\n","# Given the input image finds the location of the faces and returns the \n","#  corrisponding array of cropped face images\n","def predict_faces(filename):\n","\tpixels = plt.imread(filename)\n","\tdetector = MTCNN()\n","\tfaces = detector.detect_faces(pixels)\n","\treturn crop_faces(filename, faces)\n","\n","# Given an array of face images returns the number of faces with a mask and without a mask\n","def num_of_faces(faces):\n","\tmask = 0\n","\tno_mask = 0\n","\tfor i in range(len(faces)):\n","\t\timg_array = np.asarray(faces[i])\n","\t\timg_array = tf.expand_dims(img_array, 0)#create a batch from this one image\n","\t\tpredictions = model.predict(img_array/255)\n","\t\tpredicted_class_int = np.argmax(predictions[0])\n","\t\tprint(predicted_class_int)\n","\t\tif(predicted_class_int == 1):\n","\t\t\tmask = mask + 1\n","\t\tif(predicted_class_int == 2):\n","\t\t\tno_mask = no_mask + 1\n","\treturn (mask, no_mask)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yPwZ66tXeBN7"},"source":["# Code that generates the new dataset\n","save_dir='/content/drive/MyDrive/POLI/Artificial Neural Networks and Deep Learning/Homeworks/Homework1/Faces/Train/NoMask'\n","test_dir='/content/drive/MyDrive/POLI/Artificial Neural Networks and Deep Learning/Homeworks/Homework1/homework1-dataset/training'\n","for i in os.listdir(test_dir):\n","  if(int(df['label'][i]) == 0):\n","    j = 0\n","    filename = os.path.join(test_dir, i)\n","    faces = predict_faces(filename)          \n","    for img in faces:\n","      filename = i.split('.')[0] + \"_\" + str(j) + \".jpg\"\n","      j+=1\n","      save_path = os.path.join(save_dir, filename)\n","      print(save_path)\n","      img.save(save_path)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Lhi88L6pJ8Ol"},"source":["## Mask Dataset"]},{"cell_type":"code","metadata":{"id":"oPtpliYpJ8Om"},"source":["################ Settings ################\n","# Input image shape\n","img_height = 224 \n","img_width = 224\n","img_channels = 3\n","\n","# Directories\n","dataset_dir = os.path.join(cwd, 'Faces')\n","training_dir = os.path.join(dataset_dir, 'Train')\n","validation_dir = os.path.join(dataset_dir, 'Validation')\n","test_dir = os.path.join(dataset_dir, 'Test')\n","##########################################"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tN4VkkiDJ8Om"},"source":["### Training Dataset"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GWbjCI3uJ8Om","executionInfo":{"status":"ok","timestamp":1605952491483,"user_tz":-60,"elapsed":47208,"user":{"displayName":"Andrea Marcer","photoUrl":"https://lh6.googleusercontent.com/-y4YuyDO4-OI/AAAAAAAAAAI/AAAAAAAATsY/92XW3ePC6aE/s64/photo.jpg","userId":"09902568168977882946"}},"outputId":"c4aa1342-967d-4ca9-b960-5a6dff53af40"},"source":["################ Settings ################\n","apply_data_augmentation = True\n","batch_size = 64\n","num_classes = 3\n","##########################################\n","\n","# Create training ImageDataGenerator object\n","if apply_data_augmentation:\n","    train_data_gen = ImageDataGenerator(\n","        rotation_range=10,\n","        width_shift_range=5,\n","        height_shift_range=5,\n","        zoom_range=0.1,\n","        horizontal_flip=True,\n","        # vertical_flip=False,\n","        fill_mode='nearest',\n","        cval=0,\n","        rescale=1./255)\n","else:\n","    train_data_gen = ImageDataGenerator(rescale=1./255)\n","\n","train_gen = train_data_gen.flow_from_directory(\n","      training_dir,\n","      batch_size=batch_size,\n","      class_mode='categorical',\n","      shuffle=True,\n","      seed=SEED,\n","      target_size = (img_height, img_width)\n",")\n","\n","train_dataset = tf.data.Dataset.from_generator(\n","      lambda: train_gen,\n","      output_types=(tf.float32, tf.float32),\n","      output_shapes=([None, img_height, img_width, img_channels], [None, num_classes])\n",")\n","\n","train_dataset = train_dataset.repeat()\n","\n","train_gen.class_indices"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Found 14693 images belonging to 3 classes.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["{'Background': 0, 'Mask': 1, 'NoMask': 2}"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"riLaYRkHJ8On"},"source":["### Validation Dataset"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q01k-OlKJ8On","executionInfo":{"status":"ok","timestamp":1605952495949,"user_tz":-60,"elapsed":28058,"user":{"displayName":"Andrea Marcer","photoUrl":"https://lh6.googleusercontent.com/-y4YuyDO4-OI/AAAAAAAAAAI/AAAAAAAATsY/92XW3ePC6aE/s64/photo.jpg","userId":"09902568168977882946"}},"outputId":"8229b6b5-2844-423c-f109-346819837e50"},"source":["valid_data_gen = ImageDataGenerator(rescale=1./255)\n","\n","valid_gen = valid_data_gen.flow_from_directory(\n","      validation_dir,\n","      batch_size=batch_size, \n","      class_mode='categorical',\n","      shuffle=False,\n","      seed=SEED,\n","      target_size = (img_height, img_width)\n",")\n","\n","valid_dataset = tf.data.Dataset.from_generator(\n","      lambda: valid_gen, \n","      output_types=(tf.float32, tf.float32),\n","      output_shapes=([None, img_height, img_width, img_channels], [None, num_classes])\n",")\n","\n","valid_dataset = valid_dataset.repeat()\n","\n","valid_gen.class_indices"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Found 1960 images belonging to 3 classes.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["{'Background': 0, 'Mask': 1, 'NoMask': 2}"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"markdown","metadata":{"id":"RLAB16aCJ8On"},"source":["### Test Dataset"]},{"cell_type":"code","metadata":{"id":"p_dAObXBJ8On"},"source":["test_data_gen = ImageDataGenerator(rescale=1./255)\n","\n","test_gen = test_data_gen.flow_from_directory(\n","      '/content/drive/MyDrive/POLI/Artificial Neural Networks and Deep Learning/Homeworks/Homework1/homework1-dataset/train',\n","      batch_size=batch_size, \n","      class_mode='categorical',\n","      shuffle=False,\n","      seed=SEED,\n","      target_size = (img_height, img_width)\n",")\n","\n","test_dataset = tf.data.Dataset.from_generator(\n","      lambda: test_gen,\n","      output_types=(tf.float32, tf.float32),\n","      output_shapes=([None, img_height, img_width, img_channels], [None, num_classes])\n",")\n","\n","test_dataset = test_dataset.repeat()\n","\n","test_gen.class_indices"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ch2Ebx3MJ8On"},"source":["## Model"]},{"cell_type":"code","metadata":{"id":"1VanAv6NJ8On"},"source":["NASNet = tf.keras.applications.NASNetMobile(\n","    input_shape=(224, 224, 3), \n","    include_top=False, \n","    weights='imagenet'\n",")\n","\n","enable_finetuning = True\n","\n","if enable_finetuning:\n","    freeze_until = 15 # Layer from which we want to fine-tune.\n","    for layer in NASNet.layers[:freeze_until]:\n","        layer.trainable = False\n","else:\n","    NASNet.trainable = False\n","\n","model = tf.keras.Sequential()\n","model.add(NASNet)\n","model.add(tf.keras.layers.Flatten())\n","model.add(Dropout(0.3))\n","model.add(tf.keras.layers.Dense(units=256, activation='selu'))\n","model.add(Dropout(0.3))\n","model.add(tf.keras.layers.Dense(units=num_classes, activation='softmax'))\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W3EPqQLnJ8On"},"source":["## Training"]},{"cell_type":"code","metadata":{"id":"evlNkh1SJ8On"},"source":["################ Optimization params ################ \n","loss = tf.keras.losses.CategoricalCrossentropy()\n","learning_rate =1e-5\n","optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n","metrics = ['accuracy']\n","#####################################################\n","\n","model.compile(optimizer=optimizer, loss=loss, metrics=metrics)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kdV2KLu4J8On"},"source":["################ Settings ################\n","# Tensorboard\n","tensorBoard = True\n","\n","# Early stopping\n","earlyStopping = True\n","patience = 10\n","\n","# Model check\n","checkpoints_dir = os.path.join(cwd, 'checkpoints')\n","modelCheckpoint = False\n","##########################################\n","\n","callback_list = []\n","\n","if (earlyStopping):\n","  es = EarlyStopping(\n","      monitor = 'val_loss',\n","      min_delta = 0, \n","      patience = patience,\n","      verbose = verbose,\n","      mode = 'auto',\n","      baseline = None,\n","      restore_best_weights = True)\n","  callback_list.append(es)\n","\n","if (modelCheckpoint):\n","  cp = ModelCheckpoint(\n","      checkpoints_dir,\n","      monitor = 'val_loss',\n","      verbose = verbose,\n","      save_best_only = False,\n","      save_weights_only = False,\n","      mode = 'auto',\n","      save_freq = 1)\n","  callback_list.append(cp)\n","\n","if (tensorBoard):\n","  tb = TensorBoard(\n","      log_dir='/content/tb_log',\n","      profile_batch=0,\n","      histogram_freq=1, # if 1 shows weights histograms\n","      ) \n","  callback_list.append(tb)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MdfrakD1J8On"},"source":["################ Settings ################\n","epochs = 300\n","##########################################\n"," \n","model.fit(\n","    x = train_dataset,\n","    epochs = epochs,\n","    callbacks = callback_list,\n","    steps_per_epoch = len(train_gen), \n","    validation_data = valid_dataset,\n","    validation_steps = len(valid_gen)\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"S-L1SIXxq7rz"},"source":["## Create the prediction csv file"]},{"cell_type":"code","metadata":{"id":"Ft0H71Yx7zTm"},"source":["results = {}\n","correct = 0\n","incorrect = 0\n","test_dir='/content/drive/MyDrive/POLI/Artificial Neural Networks and Deep Learning/Homeworks/Homework1/Faces/Validation/Background'\n","for i in os.listdir(test_dir):\n","  filename = os.path.join(test_dir, i)\n","  if(int(df['label'][i]) != 0):\n","    faces = predict_faces(filename)\n","    (mask, no_mask) = num_of_faces(faces)\n","    if(mask != 0 and no_mask != 0):\n","      predicted_class_int = 2\n","    else:\n","      if(mask == 0 and no_mask == 0):\n","        predicted_class_int = 0\n","      else:\n","        if(no_mask == 0):\n","          predicted_class_int = 1\n","        else:\n","          predicted_class_int = 0\n","          \n","    results[i] = predicted_class_int"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"H2Z7L_lx7ZXF"},"source":["def create_csv(results, results_dir='./'):\n","\n","    csvfname = 'results'\n","    csvfname += datetime.now().strftime('%b%d%H-%M-%S') + '.csv'\n","\n","    with open(os.path.join(results_dir, csvfname), 'w') as f:\n","\n","        f.write('Id,Category\\n')\n","\n","        for key, value in results.items():\n","            f.write(key + ',' + str(value) + '\\n')\n","\n","create_csv(results)\n","from google.colab import files"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uM1VpGiARMty"},"source":["# Utils"]},{"cell_type":"markdown","metadata":{"id":"FcCDbAgORgJS"},"source":["## Tensorboard"]},{"cell_type":"code","metadata":{"id":"t9aGlDrhRdDA"},"source":["%load_ext tensorboard\n","%tensorboard --logdir /content/tb_log"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CsV8aUFRf1s2"},"source":["## Load model"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"93jo3lv-RNI-","executionInfo":{"status":"ok","timestamp":1605952621965,"user_tz":-60,"elapsed":126010,"user":{"displayName":"Andrea Marcer","photoUrl":"https://lh6.googleusercontent.com/-y4YuyDO4-OI/AAAAAAAAAAI/AAAAAAAATsY/92XW3ePC6aE/s64/photo.jpg","userId":"09902568168977882946"}},"outputId":"71ee1598-bdf7-4c42-fd69-b1edb4da5af6"},"source":["################ LOAD MODEL ################ \n","load_dir = os.path.join(cwd, 'model_VeryGood')\n","############################################\n","\n","model = tf.keras.models.load_model(load_dir, compile = True)\n","model.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","NASNet (Functional)          (None, 7, 7, 1056)        4269716   \n","_________________________________________________________________\n","flatten (Flatten)            (None, 51744)             0         \n","_________________________________________________________________\n","dropout (Dropout)            (None, 51744)             0         \n","_________________________________________________________________\n","dense (Dense)                (None, 256)               13246720  \n","_________________________________________________________________\n","dropout_1 (Dropout)          (None, 256)               0         \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 3)                 771       \n","=================================================================\n","Total params: 17,517,207\n","Trainable params: 17,480,469\n","Non-trainable params: 36,738\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"sjkWjZuvf8ap"},"source":["## Save Model"]},{"cell_type":"code","metadata":{"id":"ppVzoOlXTOD2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1605959877819,"user_tz":-60,"elapsed":88522,"user":{"displayName":"Andrea Marcer","photoUrl":"https://lh6.googleusercontent.com/-y4YuyDO4-OI/AAAAAAAAAAI/AAAAAAAATsY/92XW3ePC6aE/s64/photo.jpg","userId":"09902568168977882946"}},"outputId":"19aaeb26-5409-433a-9813-40e16554ff03"},"source":["################ SAVE MODEL ################ \n","save_dir = os.path.join(cwd, 'model_ThereIsMask_2')\n","############################################\n","\n","model.save(save_dir)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n","INFO:tensorflow:Assets written to: /content/drive/MyDrive/POLI/Artificial Neural Networks and Deep Learning/Homeworks/Homework1/model_ThereIsMask_2/assets\n"],"name":"stdout"}]}]}